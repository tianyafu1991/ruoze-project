# 翻山一总结
## 编译
```
源码编译不过，2个错误：
(1)有2个文件的编码为GBK，这个将2个文件转成UTF-8之后还是编译不过，最终重写了J总添加的代码后不报错
(2)有2个文件没有licenses 报以下错误,最终在这2个文件上添加licenses注释，就可以编译了
Unapproved licenses:
```

## 源码远程调试
```
在FLUME_HOME/bin/flume-ng脚本中修改：
JAVA_OPTS="-Xmx20m -Xdebug -Xrunjdwp:transport=dt_socket,address=5005,server=y,suspend=y"
开放5005端口调试

(1)、如果只发送1条日志，在Kafka中没有消费到，因为日志被缓存在Flume中tmpevent中，直到下次有带日志级别的日志进来后，才会发送到Kafka
(2)、主要通过tmpevent和tmpremainlines这两个变量，实现被折断的日志拼接成一条日志
```

## 测试代码 Java一遍、Scala一遍
```
(1)、java.io.NotSerializableException: org.apache.kafka.clients.consumer.ConsumerRecord
这个很好解决，开启kryo并注册即可
(2)、foreachRDD中获取SparkSession，Scala版本的SparkSession需要通过
    SparkSession.builder().config(rdd.sparkContext.getConf).getOrCreate()
    来获取，而Java版的可以直接在上面统一获得
```

## 处理JSON数据
```
Scala中
可以使用Spark中自带的org.json4s.jackson.Json来处理，需要导入一个隐式转换
val value = Json(DefaultFormats).parse(x.value())
val log: CDHLog = value.extract[CDHLog]
```

# 翻山二

## grafana出图表，链路走通
```
被自己小坑了一把
写入到InfluxDB时，count写成了String类型，导致Grafana出图时，对count求sum类型转换报错，图出不来
换成数值类型写入InfluxDB就可以了
```

## Java一遍、Scala一遍
```
这个没有特别需要注意的地方
```

## 优化collectAsList()
```
collectAsList()会把数据拉取到Driver端
使用foreachPartition来解决，在foreachPartition中使用BatchPoints批量的方式写入到InfluxDB中
InfluxDB相关的API直接在github官网找
https://github.com/influxdata/influxdb-java/blob/master/src/test/java/org/influxdb/InfluxDBTest.java
```



