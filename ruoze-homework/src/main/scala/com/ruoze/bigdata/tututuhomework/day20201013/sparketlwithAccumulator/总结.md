# 总结
```
1.主要得知道CDH的Hadoop、Hive、Spark的Home在哪
HADOOP_HOME=/opt/cloudera/parcels/CDH/lib/hadoop
HIVE_HOME=/opt/cloudera/parcels/CDH/lib/hive
SPARK_HOME=/opt/cloudera/parcels/SPARK2/lib/spark2

2.要配置SPARK_HOME环境变量，否则找不到scala等一些包

3.报 java.lang.ArrayIndexOutOfBoundsException: 17418这个错，需要添加一个jar包，
  参考https://stackoverflow.com/questions/53787624/spark-throwing-arrayindexoutofboundsexception-when-parallelizing-list

4.CDH的组件的jar包在/opt/cloudera/parcels/CDH/jars目录下，Spark2的在$SPARK_HOME/jars下
但是一开始图省事，直接将这2个目录下的jar包都加到azkaban的classpath中 引起jar包冲突 报javax.servlet.FilterRegistration 不匹配，明显的jar包冲突了
所以只能一个一个加
后来的做法是将jar包软连接到Azkaban的安装目录下的extlib/下,在bin/internal/internal-start-executor.sh中显式指定才生效
CLASSPATH=$CLASSPATH:/application/azkaban/azkaban-exec-server/extlib/*

```

# Azkaban配置
```
config:
  user.to.proxy: tianyafu

nodes:
  - name: spark_type_job
    type: spark
    config:
      master: yarn
      execution-jar: ruoze-homework-1.0.jar
      class: com.ruoze.bigdata.tututuhomework.day20201013.sparketlwithAccumulator.etl.SparkETL2
      deploy-mode: client
      name: spark_log_etl_on_cdh
      jars: ip2region-1.7.2.jar,paranamer-2.8.jar,ip2region.db,mysql-connector-java-5.1.47.jar,scala-library-2.12.10.jar
      driver-memory: 2048M
      executor-memory: 2048M
      executor-cores: 2
      num-executors: 2
      conf.spark.testing.memory: 2147480000
      conf.spark.dw.raw.path: /tmp/ruozedata/dw/raw/access
      conf.spark.dw.tmp.path: /tmp/ruozedata/dw/ods_tmp/access
      conf.spark.dw.ods.path: /tmp/ruozedata/dw/ods/access
      conf.spark.dw.ods.access.path: /tmp/ruozedata/dw/ods/spark_access
      conf.spark.execute.time: ${dt}
  - name: hive_add_partition
    type: hive
    dependsOn:
      - spark_type_job
    config:
      user.to.proxy: hdfs
      hive.script: hive.hql
      hiveconf.dt: ${dt}
  - name: hive_etl_job
    type: hive
    dependsOn:
      - hive_add_partition
    config: 
      hive.script: hive_etl.hql
      hiveconf.dt: ${dt}
```